![](https://raw.githubusercontent.com/tzoltak/3502-SCC-ADR/master/belka_gorna.png)

# Przetwarzanie danych: Eksploracja tekstu (Text mining)

## Prowadzący

- mgr Agnieszka Karlińska
- dr hab. Sławomir Mandes, prof. UW

## Opis przedmiotu

Eksploracja tekstu (Text mining) to ogólna nazwa zróżnicowanych metod służących do zbierania dużych ilości danych tekstowych, ich obróbki i analizy. Są one wykorzystywane do różnych celów i w różnych obszarach – od badań naukowych poprzez badania komercyjne aż po ekspertyzy przygotowywane na zlecenie rządu. Za ich pomocą podejmowano próby przewidywania kierunku ruchu akcji czy występowania protestów politycznych. Wyszukiwarki wykorzystują analizę tekstu do selekcji najbardziej adekwatnych dla nas reklam. Zwiększenie zakresu możliwego do przebadania materiału i automatyzacja analizy otwiera również nowe możliwości badań w naukach humanistycznych i społecznych.

Metodologia eksploracji tekstu jest opracowywana w różnych dyscyplinach do różnych zastosowań, w związku z czym ma wiele odmian. W ramach zajęć poznamy i przećwiczymy jej najważniejsze elementy: wyszukiwanie informacji (metody pozyskiwania tekstów), aplikację zaawansowanych metod statystycznych i sposobów przetwarzania języka naturalnego (NLP), takich jak tagowanie części mowy i analiza składniowa, wykorzystanie technik statystycznych do identyfikowania w tekście osób, organizacji i nazw miejsc oraz analizę sentymentu (sentiment analysis), która obejmuje rozpoznanie emocjonalnego zabarwienia analizowanego tekstu i wnioskowanie na tej podstawie o jego znaczeniu. Eksploracja tekstu obejmuje również bardziej podstawowe techniki pozyskiwania i przetwarzania danych. Należą do nich web scraping i web crawling oraz wykorzystywanie słowników i innych zasobów leksykalnych do przetwarzania tekstów.

Uczestnicy i uczestniczki zajęć poznają narzędzia do przetwarzania danych tekstowych z wielu plików zapisanych w różnych formatach oraz zautomatyzowane sposoby wydobywania informacji z tekstów, dowiedzą się, jak przygotować proste statystyki dotyczące badanych dokumentów oraz wyznaczyć podobieństwo i pogrupować teksty, a rezultaty odczytać z wykorzystaniem różnych metod wizualizacji wyników.

Na zajęciach korzystać będziemy z narzędzi do przetwarzania i analizy danych tekstowych Korpusomat, TermoPL (IPI PAN), Inforex, WSD, Topic, WebSty, Sentemo (CLARIN-PL), AntConc, LancsBox, bibliotek Natural Language Toolkit (NLTK) i spaCy, pracujących w środowisku Pythona, oraz bibliotek OpenNLP, tm i stylo pracujących w języku R.

---
Materiały na zajęcia *Analiza danych ilościowych z wykorzystaniem R* zostały przygotowane w ramach projektu *Program zintegrowanych działań na rzecz rozwoju Uniwersytetu Warszawskiego*, realizowanego w ramach programu operacyjnego Wiedza Edukacja Rozwój, oś priorytetowa III. *Szkolnictwo wyższe dla gospodarki i rozwoju*, działanie: 3.5 *Kompleksowe programy szkół wyższych*.

![](https://raw.githubusercontent.com/tzoltak/3502-SCC-ADR/master/belka_dolna.png)

