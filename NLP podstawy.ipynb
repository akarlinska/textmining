{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przetwarzanie języka naturalnego (ang. natural language processing, NLP)\n",
    "\n",
    "**Autorzy:** S. Mandes, A. Karlińska\n",
    "## Wprowadzanie \n",
    "\n",
    "### Co to jest NLP?\n",
    "\n",
    "\"Interdyscyplinarna dziedzina, łącząca zagadnienia sztucznej inteligencji i językoznawstwa, zajmująca się automatyzacją analizy, rozumienia, tłumaczenia i generowania języka naturalnego przez komputer.\" (źródło: wikipedia pl --> polecam cały wpis. A jeszcze lepiej wpis na eng wiki)\n",
    "\n",
    "A mówiąc bardziej konkretnie:\n",
    "\n",
    "NLP rozwija się jako odpowiedź na problemy: a) przetwarzania, b) klasyfikowania i c) analizy \"treści\" bardzo dużych zbiorów tekstów w formie elektronicznej. \n",
    "\n",
    "Ad. a) kiedy wpisujemy jakiś tekst w wyszukiwarkę, to odpowiedź wyświetla się błyskawicznie. Czyli nie jest tak, że po wpisaniu zapytania jakiś program przeszukuje dla nas odpowiedzi na pytanie. Ono już jest (zazwyczaj) gotowe. Zawartość internetu (pewna jego część, tak naprawdę) została przetworzona pod kątem potencjalnych pytań / wyszukiwań. Problem pierwszy to, jak przetwarzać tekst, biorąc pod uwagę, że komputer przetwarza liczby. Jak przetwarzać efektywnie? itd.\n",
    "\n",
    "Ad. b) kiedy wyszukujemy jakiś produkt, to otrzymujemy od razu jakieś sugestie. To samo dotyczy filmów, książek, wszystkiego. Problem drugi polega na grupowaniu treści w większe całości (sieci znaczeń), które umożliwią \"wnioskowanie\". Inny klasyczny problem: jak rozpoznać, że coś jest spamem, a coś nie? \n",
    "\n",
    "Ad. c) jak rozpoznać, o czym jest mowa w tekście? Jaki jest emocjonalny wydźwięk tekstu? Jak rozpoznać treści pytania, które kierujemy do Alexy / Siri / itd. (chat bot)? Obecnie główny problem: jak rozpoznać fake newsy?\n",
    "\n",
    "### A jak to ma się do socjologii?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Materiały dodatkowe:\n",
    "\n",
    "SpaCy:\n",
    "https://spacy.io/usage/spacy-101#_title\n",
    "\n",
    "Dlaczego język polski trudnym jest:\n",
    "\n",
    "https://www.youtube.com/watch?v=WY6zrqF9q-w&t=1824s\n",
    "(1 połowa - wystąpienie Ł. Kobylińskiego)\n",
    "\n",
    "O NLP - trochę historii i co się dzieje współcześnie:\n",
    "\n",
    "https://www.youtube.com/watch?v=IUbFMt_4_Hw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tematy zajęć:\n",
    "\n",
    "- SpaCy - podstawy\n",
    "- tokenizacja\n",
    "- stemming\n",
    "- lematyzacja\n",
    "- stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Praca z SpaCy w Pythonie\n",
    "\n",
    "Jest to typowy zestaw instrukcji do importowania i pracy ze apaCy. To może zająć trochę czasu - spaCy posiada dość dużą bibliotekę do załadowania. \n",
    "\n",
    "`sm` przy modelu języka oznacza small version. To jest model, który będzie wykorzystany do analizy. Za jakiś czas użyjemy pełnego modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy Doc object i przeprowadzamy tokenizację. \n",
    "doc = nlp('Facebook is going to buy U.S. startup for $6 million')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook PROPN nsubj\n",
      "is AUX aux\n",
      "going VERB ROOT\n",
      "to PART aux\n",
      "buy VERB xcomp\n",
      "U.S. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "6 NUM compound\n",
      "million NUM pobj\n"
     ]
    }
   ],
   "source": [
    "'''Wyświetlamy każdy token odddzielnie. pos - part of speech \n",
    "(bez _ pokaże cyfry, zakodowane częsci mowy, # dep - syntactic dependency'''\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie wygląda to zbyt jasno, ale od razu widzimy, że dzieje się kilka ciekawych rzeczy:\n",
    "\n",
    "1. Facebook został rozpoznany jako nazwa własna, a nie tylko słowo na początku zdania\n",
    "2. Stany Zjednoczone zostały rozpoznane jako nazwa pomimo kropek (nazywamy to \"tokenem\")\n",
    "\n",
    "Później wyjaśnimy pozostałe skróty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Pipeline\n",
    "Kiedy uruchamiamy `nlp`, tekst przechodzi przez *processing pipeline*, który najpierw rozbija tekst na podstawowe elementy, a następnie wykonuje serię operacji, aby oznaczyć, przetworzyć i opisać dane. Co dokładnie robi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy sprawdzić, jakie elementy znajdują się obecnie w pipeline.\n",
    "Ważne, bo można sobie różne procesy do pipline wstawiać."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x27c980cc4c8>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x27c980b0ee8>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x27c980cd4c8>)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacja\n",
    "Pierwszym krokiem w przetwarzaniu tekstu jest podzielenie wszystkich części składowych (słów i interpunkcji) na \"tokeny\". Są one opatrzone adnotacjami wewnątrz obiektu Doc, które zawierają więcej informacji o poszczególnych tokenach. Więcej szczegółów na temat tokenizacji za chwilę. Na razie spójrzmy na inny przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "going VERB ROOT\n",
      "to PART aux\n",
      "buy VERB xcomp\n",
      "       SPACE \n",
      "U.S. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "6 NUM compound\n",
      "million NUM pobj\n",
      "anymore ADV advmod\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"Facebook isn't going to buy U.S. startup for $6 million anymore.\")\n",
    "\n",
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy rozpoznaje zarówno sam czasownik, jak i dołączoną do niego negację. \n",
    "\n",
    "Uwaga: zarówno rozszerzona \"biała przestrzeń\" (whitespace) jak i kropka na końcu zdania są przypisane do własnych tokenów.\n",
    "\n",
    "Należy zauważyć, że nawet jeśli doc2 zawiera przetworzone informacje o każdym tokenie, zachowuje on również oryginalny tekst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Facebook isn't going to buy       U.S. startup for $6 million anymore."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Facebook"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Part-of-Speech Tagging (POS)\n",
    "\n",
    "Następnym krokiem po podzieleniu tekstu na tokeny jest przypisanie informacji o części mowy. W powyższym przykładzie `Facebook` został uznany za ***nazwę własną***. Tutaj wymagane jest pewne statystyczne modelowanie. Na przykład,słowa, które następują po \"the\", są zazwyczaj rzeczownikami.\n",
    "\n",
    "Więcej: https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0].pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Dependencies \n",
    "\n",
    "SpaCy określa również zależności składniowe tokenów. `Facebook` jest identyfikowany jako `nsubj` lub ***nominal subject*** zdania.\n",
    "\n",
    "więcej: https://spacy.io/api/annotation#dependency-parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nsubj'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0].dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal subject'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('nsubj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co jeszcze \"ukryte\" jest w tokenach? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going\n"
     ]
    }
   ],
   "source": [
    "print(doc2[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "# lemat (podstawowa forma słowa):\n",
    "\n",
    "print(doc2[3].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n",
      "VBG / verb, gerund or present participle\n"
     ]
    }
   ],
   "source": [
    "# informacja o o tym, jaka to jest część zdania: ogólna i dokładna\n",
    "print(doc2[3].pos_)\n",
    "print(doc2[3].tag_ + ' / ' + spacy.explain(doc2[3].tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook: Xxxxx\n",
      "U.S. : X.X.\n"
     ]
    }
   ],
   "source": [
    "# Word Shapes:\n",
    "print(doc2[0].text+': '+doc2[0].shape_)\n",
    "print(doc[5].text+' : '+doc[5].shape_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Boolean Values:\n",
    "print(doc2[0].is_alpha)\n",
    "print(doc2[0].is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Spans\n",
    "\n",
    "Duże teksty mogą być czasami trudne do obróbki. **span** służy do dzielenia tekstu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(\"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, \n",
    "information engineering, and artificial intelligence concerned with the interactions between \n",
    "computers and human (natural) languages, in particular how to program computers to process and\n",
    "analyze large amounts of natural language data. The history of natural language processing (NLP) \n",
    "generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing \n",
    "published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called \n",
    "the Turing test as a criterion of intelligence.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linguistics, computer science, \n",
      "information engineering, and artificial intelligence concerned with the interactions between \n",
      "computers and\n"
     ]
    }
   ],
   "source": [
    "frag = doc3[10:30]\n",
    "print(frag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(frag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Zdania\n",
    "Niektóre tokeny wewnątrz tekstu mogą również otrzymać znacznik \"początek zdania\". Chociaż nie buduje to natychmiast listy zdań, tagi te umożliwiają generowanie segmentów zdań poprzez `Doc.sents`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a subfield of linguistics, computer science, \n",
      "information engineering, and artificial intelligence concerned with the interactions between \n",
      "computers and human (natural) languages, in particular how to program computers to process and\n",
      "analyze large amounts of natural language data. \n",
      "\n",
      "The history of natural language processing (NLP) \n",
      "generally started in the 1950s, although work can be found from earlier periods. \n",
      "\n",
      "In 1950, Alan Turing \n",
      "published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called \n",
      "the Turing test as a criterion of intelligence. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in doc3.sents:\n",
    "    print(sent, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3[0].is_sent_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tokenizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Let's go to N.Y.!\"\n"
     ]
    }
   ],
   "source": [
    "# nieco bardziej skomplikowane przypadki stringów\n",
    "txt = '\"Let\\'s go to N.Y.!\"'\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" | Let | 's | go | to | N.Y. | ! | \" | "
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, end=' | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co się dzieje pod spodem?\n",
    "\n",
    "https://spacy.io/usage/spacy-101#annotations-token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefixes, Suffixes, Infixes\n",
    "SpaCy odizoluje interpunkcję, która nie *nie* stanowi integralnej części słowa. Cudzysłowy, przecinki i interpunkcja na końcu zdania będą oddzielnymi tokenami. \n",
    "\n",
    "Ale nie zawsze. Interpunkcja, która istnieje jako część adresu e-mail, strony internetowej lub wartości numerycznej, będzie zachowana jako część tokenu.\n",
    "\n",
    "- Prefix: Character(s) at the beginning, e.g. $, (, “, ¿.\n",
    "- Suffix: Character(s) at the end, e.g. km, ), ”, !.\n",
    "- Infix: Character(s) in between, e.g. -, --, /, ….\n",
    "- Exception: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied, e.g. `St. U.S.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help\n",
      "us\n",
      "!\n",
      "Send\n",
      "an\n",
      "e\n",
      "-\n",
      "mail\n",
      "support@website.com\n",
      "or\n",
      "visit\n",
      "us\n",
      "at\n",
      "http://www.website.com\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"Help us! Send an e-mail support@website.com or visit us at http://www.website.com!\")\n",
    "\n",
    "for t in doc2:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwaga: e-mail został podzielony na 3 tokeny. Czyli jak widać, nie zawsze to działa, jak byśmy chcieli lub myśleli, że powinno działać!\n",
    "\n",
    "https://english.stackexchange.com/questions/1925/email-or-e-mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokeny możemy policzyć\n",
    "len(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "!"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokeny możemy przetwarzać po indeksach\n",
    "doc2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'spacy.tokens.doc.Doc' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4de27bdd79fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# ale nie można zmieniać\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdoc2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'spacy.tokens.doc.Doc' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# ale nie można zmieniać\n",
    "doc2[2] = doc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rozpoznawanie nazw własnych\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Facebook is going to buy Polish startup for $6 million from Warsaw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polish\n",
      "$6 million\n",
      "Warsaw\n"
     ]
    }
   ],
   "source": [
    "# wydobywamy z tekstu nazwy własne. Ale też 6 milionów\n",
    "# uwaga, do nazwy dokumentu dodajemy .ents\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polish - NORP - Nationalities or religious or political groups\n",
      "$6 million - MONEY - Monetary values, including unit\n",
      "Warsaw - GPE - Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "# możemy od razu je zaklasyfikować (label, znowu z _)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun chunks (kawałki rzeczownika??)\n",
    "czyli rzeczowniki z słowami opisujacymi / określającymi je.\n",
    "\n",
    "- Text: The original noun chunk text.\n",
    "- Root text: The original text of the word connecting the noun chunk to the rest of the parse.\n",
    "- Root dep: Dependency relation connecting the root to its head.\n",
    "\n",
    "https://spacy.io/usage/linguistic-features#noun-chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc3.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars        cars                  nsubj\n",
      "insurance liability    liability             dobj\n",
      "manufacturers          manufacturers         pobj\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc3.noun_chunks:\n",
    "    print(f'{chunk.text:{22}} {chunk.root.text:{21}} {chunk.root.dep_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Built-in Visualizers\n",
    "\n",
    "spaCy posiada narzędzie do wizualizacji relacji pomiędzy tokenami **displaCy**. \n",
    "\n",
    "Uwaga: najlepiej działa w Jupyter. Można też robić w innych narzędziach, ale wymaga eksportu do HTML.\n",
    "\n",
    "\n",
    "Więcej informacji: https://spacy.io/usage/visualizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    the last quarter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    nearly 20 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    iPods\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " for a profit of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    $6 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp('Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Stemming\n",
    "\n",
    "W wyszukiwaniu informacji oraz w morfologii (w językoznawstwie) jest to proces usunięcia ze słowa końcówki fleksyjnej, pozostawiając tylko temat wyrazu. Proces stemmingu może być przeprowadzany w celu zmierzenia popularności danego słowa. Końcówki fleksyjne zaniżają faktyczne dane. Algorytmy stemmingu są przedmiotem badań informatyki od lat 60. XX wieku. Pierwszy stemmer, czyli program do przeprowadzania procesu stemmingu, został napisany i opublikowany przez Julie Beth Lovins w 1968. W czerwcu 1980 Martin Porter opublikował swój algorytm stemmingu, zwany Algorytmem Portera.\n",
    "\n",
    "Np. angielskie słowa: „connection”, „connections”, „connective”, „connected”, „connecting” poddane stemmingowi dadzą ten sam wynik, czyli słowo „connect”.\n",
    "\n",
    "źródło: wikipedia\n",
    "\n",
    "W uproszczeniu, stemming to obcięcie wszelkiego rodzaju przedrostków i przyrostków, w celu dotarcia do „rdzenia” wyrazu. Rdzeń nie musi być poprawnym słowem.\n",
    "\n",
    "SpaCy nie posiada narzędzia do stemmingu --> por. dyskusja o zaletach stemmingu w porównaniu do lematyzacji:\n",
    "\n",
    "https://github.com/explosion/spaCy/issues/327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importujemy NLTK i całą biblitekę do stemmingu w wydaniu Portera\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = ['connection', 'connections', 'connective', 'connected', 'connecting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection --> connect\n",
      "connections --> connect\n",
      "connective --> connect\n",
      "connected --> connect\n",
      "connecting --> connect\n"
     ]
    }
   ],
   "source": [
    "for word in words1:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ale nie zawsze to działa dobrze, np problem z przysłówkami\n",
    "words2 = ['easily', 'fairly', 'quitely', 'entirely']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easily --> easili\n",
      "fairly --> fairli\n",
      "quitely --> quit\n",
      "entirely --> entir\n"
     ]
    }
   ],
   "source": [
    "for word in words2:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jest też druga wersja tzw. \"Snowball Stemmer\" zwany też \"English Stemmer\" or \"Porter2 Stemmer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection --> connect\n",
      "connections --> connect\n",
      "connective --> connect\n",
      "connected --> connect\n",
      "connecting --> connect\n"
     ]
    }
   ],
   "source": [
    "for word in words1:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easily --> easili\n",
      "fairly --> fair\n",
      "quitely --> quit\n",
      "entirely --> entir\n"
     ]
    }
   ],
   "source": [
    "for word in words2:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Lematyzacja\n",
    "\n",
    "Jest to proces sprowadzania zbioru słów do lematu (podstawowej postaci). W przypadku czasownika będzie do bezokolicznik, w przypadku rzeczownika – mianownik liczby pojedynczej. Do wykonania tego zadania potrzebny jest słownik lub rozbudowany zestaw reguł fleksyjnych dla danego języka.\n",
    "\n",
    "**Lemat** - ta spośród form gramatycznych wyrazu odmiennego, która jest tradycyjnie wykorzystywana w słownikach i reprezentuje tam w nagłówku artykułu hasłowego cały wyraz ze wszystkimi jego formami. Forma ta stanowi niejako „umowną etykietę zbioru form” i decyduje o umiejscowieniu artykułu hasłowego w słowniku.\n",
    "\n",
    "Wybór formy słownikowej bywa ustalony tradycją i różni się w zależności od języka.\n",
    "\n",
    "Forma słownikowa bywa zwykle traktowana przez osoby uczące się języka obcego jako podstawowa, tj. taka, w której przyswajane są nowe wyrazy i od której tworzy się pozostałe formy odmiany.\n",
    "                                                                  \n",
    "                                                              źródło: wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561228191312463089 \t\t -PRON-\n",
      "10382539506755952630 \t\t be\n",
      "11901859001352538922 \t\t a\n",
      "12223578186674142820 \t\t biker\n",
      "16029548483725639901 \t\t bike\n",
      "3002984154512732771 \t\t in\n",
      "11901859001352538922 \t\t a\n",
      "8048469955494714898 \t\t race\n",
      "16950148841647037698 \t\t because\n",
      "561228191312463089 \t\t -PRON-\n",
      "3702023516439754181 \t\t love\n",
      "3791531372978436496 \t\t to\n",
      "16029548483725639901 \t\t bike\n",
      "2283656566040971221 \t\t and\n",
      "561228191312463089 \t\t -PRON-\n",
      "16029548483725639901 \t\t bike\n",
      "11042482332948150395 \t\t today\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(\"I am a biker biking in a race because I love to bike and I bike today\")\n",
    "\n",
    "for token in doc4:\n",
    "    print(token.lemma, '\\t\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I          PRON         561228191312463089 -PRON-    \n",
      "am         AUX        10382539506755952630 be        \n",
      "a          DET        11901859001352538922 a         \n",
      "biker      NOUN       12223578186674142820 biker     \n",
      "biking     VERB       16029548483725639901 bike      \n",
      "in         ADP         3002984154512732771 in        \n",
      "a          DET        11901859001352538922 a         \n",
      "race       NOUN        8048469955494714898 race      \n",
      "because    SCONJ      16950148841647037698 because   \n",
      "I          PRON         561228191312463089 -PRON-    \n",
      "love       VERB        3702023516439754181 love      \n",
      "to         PART        3791531372978436496 to        \n",
      "bike       VERB       16029548483725639901 bike      \n",
      "and        CCONJ       2283656566040971221 and       \n",
      "I          PRON         561228191312463089 -PRON-    \n",
      "bike       VERB       16029548483725639901 bike      \n",
      "today      NOUN       11042482332948150395 today     \n"
     ]
    }
   ],
   "source": [
    "for lem in doc4:\n",
    "    print(f'{lem.text:{10}} {lem.pos_:{10}} {lem.lemma:{20}} {lem.lemma_:{10}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pokaż_lematy(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{10}} {token.pos_:{10}} {token.lemma:{20}} {token.lemma_:{10}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I          PRON         561228191312463089 -PRON-    \n",
      "want       VERB        7597692042947428029 want      \n",
      "to         PART        3791531372978436496 to        \n",
      "show       VERB        1916734850589852068 show      \n",
      "you        PRON         561228191312463089 -PRON-    \n",
      "something  PRON       17370494668576369452 something \n",
      "!          PUNCT      17494803046312582752 !         \n"
     ]
    }
   ],
   "source": [
    "# a co się stanie, jak użyjemy nieistniejącego słowa \n",
    "pokaż_lematy(nlp('I want to show you something!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I          PRON         561228191312463089 -PRON-    \n",
      "am         AUX        10382539506755952630 be        \n",
      "meeting    VERB        6880656908171229526 meet      \n",
      "him        PRON         561228191312463089 -PRON-    \n",
      "tomorrow   NOUN        3573583789758258062 tomorrow  \n",
      "at         ADP        11667289587015813222 at        \n",
      "the        DET         7425985699627899538 the       \n",
      "meeting    NOUN       14798207169164081740 meeting   \n",
      ".          PUNCT      12646065887601541794 .         \n"
     ]
    }
   ],
   "source": [
    "# mamy w poniższym zdaniu dwa razy meeting\n",
    "doc5 = nlp(\"I am meeting him tomorrow at the meeting.\")\n",
    "\n",
    "pokaż_lematy(doc5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
